{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import hashlib\n",
    "import shutil\n",
    "import readability\n",
    "import logging\n",
    "\n",
    "# Set of visited URLs to prevent infinite recursion\n",
    "visited_urls = set()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def download_page(url, timeout=10):\n",
    "    \"\"\"\n",
    "    Downloads the content of a web page from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the web page to download.\n",
    "        timeout (int): Timeout duration in seconds.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the web page as a string, or None if there was an error.\n",
    "\n",
    "    Raises:\n",
    "        requests.RequestException: If there was an error while downloading the web page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        response.raise_for_status()  # Raises an error for bad status codes\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        logging.warning(f\"Error downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls(html, base_url, ignored_extensions=None):\n",
    "    \"\"\"\n",
    "    Extracts all URLs from the given HTML content, resolving relative URLs and ignoring hash fragments.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content to extract URLs from.\n",
    "        base_url (str): The base URL used to resolve relative URLs.\n",
    "        ignored_extensions (list): List of file extensions to ignore.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of URLs extracted from the HTML content.\n",
    "    \"\"\"\n",
    "    if ignored_extensions is None:\n",
    "        ignored_extensions = ['.txt', '.pdf', '.docx']\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    urls = set()\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        # Resolve relative URLs and filter by hash fragment\n",
    "        full_url = urljoin(base_url, href.split('#', 1)[0])\n",
    "        # Ignore URLs ending with specific file extensions\n",
    "        if any(full_url.endswith(ext) for ext in ignored_extensions):\n",
    "            continue\n",
    "        if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "            urls.add(full_url)\n",
    "    return urls\n",
    "\n",
    "def html_to_markdown(html):\n",
    "    \"\"\"\n",
    "    Converts HTML content to Markdown format.\n",
    "\n",
    "    Parameters:\n",
    "    html (str): The HTML content to be converted.\n",
    "\n",
    "    Returns:\n",
    "    str: The Markdown representation of the HTML content.\n",
    "    \"\"\"\n",
    "    # Using readability to extract the main content\n",
    "    document = readability.Document(html)\n",
    "    summary = document.summary()\n",
    "\n",
    "    converter = html2text.HTML2Text()\n",
    "    converter.ignore_links = False\n",
    "    return converter.handle(summary)\n",
    "\n",
    "def save_markdown(markdown, folder, filename):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown)\n",
    "\n",
    "def generate_filename(url, base_url):\n",
    "    \"\"\"\n",
    "    Generate a filename based on the given URL and base URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL from which the filename will be generated.\n",
    "        base_url (str): The base URL used to remove the common path from the URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated filename.\n",
    "    \"\"\"\n",
    "    # Parse the URLs\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "\n",
    "    # Remove the base URL path to get the unique part of the path\n",
    "    base_path = parsed_base_url.path.strip('/')\n",
    "    unique_path = parsed_url.path.strip('/')\n",
    "\n",
    "    # If the base_path is not empty, remove it from the start of unique_path\n",
    "    if base_path and unique_path.startswith(base_path):\n",
    "        unique_path = unique_path[len(base_path):].strip('/')\n",
    "\n",
    "    # Split the path into segments and join them with hyphens\n",
    "    if unique_path:\n",
    "        filename = unique_path.replace('/', '-').lower() + \".md\"\n",
    "    else:\n",
    "        filename = \"index.md\"\n",
    "    return filename\n",
    "\n",
    "def scrape_site(url, base_url, base_folder='', depth_limit=3, current_depth=0):\n",
    "    \"\"\"\n",
    "    Scrapes a website recursively, saving the content as markdown files.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        base_url (str): The base URL of the website.\n",
    "        base_folder (str, optional): The base folder to save the markdown files. Defaults to ''.\n",
    "        depth_limit (int, optional): Maximum recursion depth. Defaults to 3.\n",
    "        current_depth (int, optional): Current recursion depth. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the URL starts with the base URL\n",
    "    if not url.startswith(base_url):\n",
    "        return\n",
    "\n",
    "    if url in visited_urls or urlparse(url).netloc != urlparse(base_url).netloc:\n",
    "        return\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    logging.info(f\"Scraping {url} (depth: {current_depth})\")\n",
    "    html = download_page(url)\n",
    "    if html:\n",
    "        markdown = html_to_markdown(html)\n",
    "        filename = generate_filename(url, base_url)\n",
    "        folder = os.path.join(base_folder, urlparse(base_url).netloc)\n",
    "        save_markdown(markdown, folder, filename)\n",
    "\n",
    "        # Recursively scrape the extracted URLs up to the depth limit\n",
    "        if current_depth < depth_limit:\n",
    "            for link in extract_urls(html, url):\n",
    "                scrape_site(link, base_url, base_folder, depth_limit, current_depth + 1)\n",
    "\n",
    "def clean_directory(folder):\n",
    "    \"\"\"\n",
    "    Deletes all files and folders in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the directory to be cleaned.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there is an error while deleting files or folders.\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                logging.error(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# Example usage\n",
    "base_url = 'https://docs.alpaca.markets/'  # Change this URL to your target\n",
    "start_url = f'{base_url}'  # Change this URL to your target\n",
    "base_folder = 'output'\n",
    "\n",
    "clean_directory(base_folder)\n",
    "scrape_site(start_url, start_url, base_folder, depth_limit=3)\n",
    "logging.info(\"Scraping complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
