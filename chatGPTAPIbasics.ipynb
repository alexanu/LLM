{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import os\n",
    "import httpx\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # show several outputs in jupyter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['HTTPS_PROXY'] = 'http://de001-surf.zone2.proxy.allianz:8080'\n",
    "#os.environ['HTTP_PROXY'] = 'http://de001-surf.zone2.proxy.allianz:8080'\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "# openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT_FEES']\n",
    "# openai.api_key = os.environ['AZURE_OPENAI_API_KEY_FEES']\n",
    "\n",
    "openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n",
    "\n",
    "#os.environ['HTTPS_PROXY'] = ''\n",
    "#print(os.environ['HTTP_PROXY'])\n",
    "DefaultModel = 'gpt-4o' # the scope of available models depends on deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT_FEES']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY_FEES']\n",
    "DefaultModel = 'gpt-4o' # the scope of available models depends on deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4-turbo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OpenAIModels = {\n",
    "    'gpt-4o': {'input': 5.00, 'output': 15.00, 'context_window': 128, 'training': 'Oct23', 'output': 4, 'desc': 'Text and image input, text output'},\n",
    "    'gpt-4-turbo': {'input': 10.00, 'output': 30.00, 'context_window': 128, 'training': 'Dec23', 'output': 4, 'desc': 'Text and image input, text output'},\n",
    "    'gpt-3.5-turbo': {'input': 0.5, 'output': 1.5, 'context_window': 16, 'training': 'Sep21', 'output': 4, 'desc': 'Text input, text output'},\n",
    "    'text-embedding-3-small': {'input': 0.02, 'output': 0, 'context_window': 16, 'training': '', 'output': 1.5, 'desc': 'convert text into a numerical form'},\n",
    "    'text-embedding-3-large': {'input': 0.13, 'output': 0, 'context_window': 16, 'training': '', 'output': 3, 'desc': 'convert text into a numerical form'},\n",
    "}\n",
    "ModelNames=list(OpenAIModels.keys())\n",
    "DefaultModel = ModelNames[1] # gpt-3.5-turbo\n",
    "DefaultModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 roles:\n",
    "\n",
    "- User - This is meant to mimic the end-user that is interacting with the assistant. This is the role that you will be using most of the time.\n",
    "- System - This role can mimic sort of background nudges and prompts that you might want to inject into the conversation, but that dont need a response. At the moment, system is weighted less than \"user,\" so it still seems more useful to use the user for encouraging specific behaviors in my opinion.\n",
    "- Assistant - This is the agent's response. Often this will be actual responses, but keep in mind... you will be able to inject your own responses here, so you can actually have the agent say whatever you want. This is a bit of a hack, but it's a fun one and can be useful in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the ChatGPT API works is you need to query the model. Since these models often make use of chat history/context, every query needs to, or can, include a full message history context. \n",
    "\n",
    "Keep in mind, however that the maximum context length (XXXX tokens), so you need to stay under that. There are lots of options to work around this, the simplest being truncating earlier messages or to summarize and condense the previous message history. You might use a database or some other storage method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = []\n",
    "\n",
    "def chat(user_input, role=\"user\", model=DefaultModel):\n",
    "    message_history.append({\"role\": role, \"content\": f\"{user_input}\"})\n",
    "    completion = client.chat.completions.create(messages=message_history, model=model)\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})\n",
    "    return reply_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold different scenarios and their estimated output-to-input token ratios\n",
    "token_ratio_estimates = {\n",
    "    'email_response': 1.5,    # Outputs are generally 1.5 times longer than the inputs\n",
    "    'content_summary': 0.8,   # Summaries are usually shorter than the original content\n",
    "    'factual_answer': 0.5,    # Direct answers to factual questions tend to be concise\n",
    "    'creative_story': 3.0,    # Creative stories may be much longer than the initial prompts\n",
    "    'detailed_explanation': 20,  # Detailed explanations or complex answers can be much longer\n",
    "    'limit': 0  # This will be treated specially to limit output tokens\n",
    "}\n",
    "\n",
    "# Maximum tokens for the 'limit' scenario\n",
    "max_output_tokens = 100\n",
    "\n",
    "def calculate_cost(text, model_name = DefaultModel, scenario='limit'):\n",
    "\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    input_tokens = enc.encode(text)   \n",
    "    input_tokens_count = len(input_tokens) \n",
    "\n",
    "    # Determine the output token count based on the scenario\n",
    "    if scenario == 'limit':\n",
    "        output_token_count = max_output_tokens\n",
    "    else:\n",
    "        ratio = token_ratio_estimates.get(scenario, 1)  # Use 1 as a default ratio if the scenario is not found\n",
    "        output_token_count = int(input_tokens_count * ratio)\n",
    "    \n",
    "    # Calculate the total token count (input + estimated output)\n",
    "    total_token_count = input_tokens_count + output_token_count\n",
    "    \n",
    "    # Retrieve cost per million tokens for the model, assuming it can be a dictionary or a single value\n",
    "    cost_per_million = ModelCosts[model_name]\n",
    "    if isinstance(cost_per_million, dict):\n",
    "        # Assuming the model has separate costs for input and output, typically not the case but for example\n",
    "        input_cost_per_million = cost_per_million.get('input', 0)\n",
    "        output_cost_per_million = cost_per_million.get('output', 0)\n",
    "        total_cost = (input_tokens_count / 1_000_000) * input_cost_per_million + (output_token_count / 1_000_000) * output_cost_per_million\n",
    "    else:\n",
    "        total_cost = (total_token_count / 1_000_000) * cost_per_million\n",
    "    \n",
    "    return round(total_cost,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Schema output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9gtWawsMOOE0XTzBUwyGgpGkdpTmH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"world_series_winner\": \"Los Angeles Dodgers\",\\n  \"year\": 2020\\n}', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1720010240, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_abc28019ad', usage=CompletionUsage(completion_tokens=22, prompt_tokens=41, total_tokens=63), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"world_series_winner\": \"Los Angeles Dodgers\",\\n  \"year\": 2020\\n}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"}, # When using JSON mode, always instruct the model to produce JSON via some message in the conversation\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "]\n",
    "response = openai.chat.completions.create(\n",
    "                model=DefaultModel,\n",
    "                messages=messages,\n",
    "                response_format={ \"type\": \"json_object\" }, # When using JSON mode, always instruct the model to produce JSON via some message in the conversation\n",
    "                # seed = 1, # set it to any integer of your choice and use the same value across requests you'd like deterministic outputs for\n",
    "                # temperature=0.5,\n",
    "                # max_tokens=200\n",
    "            )\n",
    "response\n",
    "response.usage.total_tokens\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every response will include a finish_reason. The possible values for finish_reason are:\n",
    "- 'stop': API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n",
    "- 'length': Incomplete model output due to max_tokens parameter or token limit\n",
    "- 'function_call': The model decided to call a function\n",
    "- 'content_filter': Omitted content due to a flag from our content filters\n",
    "- 'null': API response still in progress or incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Date from Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31.12.2012'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n",
    "\n",
    "\n",
    "sys_prompt = '''\n",
    "Please read the filename provided and extract the end date of the quarter in the format DD.MM.YYYY  . The filename will typically indicate the quarter and year, but the name of the file could also have month and a year. Use the following examples for reference:\n",
    "\n",
    "1. 'Fees Q3 2023 Valida corrected.xlsx' should return 30.09.2023\n",
    "2. 'talktalktal 2022Q4_dummytext_not_rrr.xlsx' should return 31.12.2022\n",
    "3. 'Q22023_bla-bla-bla_blaxxxx.xlsx' should return 30.06.2023\n",
    "4. 'dfa fsd sds s September 22' should return 30.09.2022\n",
    "5. 'Somedata_052015_blaxxxx.xlsx' should return 31.05.2015\n",
    "6. 'Data excteract Mar 2021 final' should return 31.03.2021\n",
    "\n",
    "Identify the quarter and year from the filename and return the corresponding end date of that quarter.\n",
    "'''\n",
    "\n",
    "name_of_file = 'sdfsfdsfd2012Q4_dummytext_not_rrr.xlsx'\n",
    "#name_of_file = 'F 2014 Q1 Valida corrected.xlsx'\n",
    "#name_of_file = 'File corrected Mar 2022.xlsx'\n",
    "\n",
    "messages = [\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": name_of_file}\n",
    "            ]\n",
    "# os.environ['HTTPS_PROXY'] = ''\n",
    "response = openai.chat.completions.create(\n",
    "                model='gpt-4',\n",
    "                messages=messages,\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            )\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
