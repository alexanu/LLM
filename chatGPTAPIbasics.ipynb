{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import os\n",
    "import httpx\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # show several outputs in jupyter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['HTTPS_PROXY'] = 'http://de001-surf.zone2.proxy.allianz:8080'\n",
    "#os.environ['HTTP_PROXY'] = 'http://de001-surf.zone2.proxy.allianz:8080'\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "# openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT_FEES']\n",
    "# openai.api_key = os.environ['AZURE_OPENAI_API_KEY_FEES']\n",
    "\n",
    "openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n",
    "\n",
    "#os.environ['HTTPS_PROXY'] = ''\n",
    "#print(os.environ['HTTP_PROXY'])\n",
    "DefaultModel = 'gpt-4o' # the scope of available models depends on deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT_FEES']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY_FEES']\n",
    "DefaultModel = 'gpt-4o' # the scope of available models depends on deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4-turbo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OpenAIModels = {\n",
    "    'gpt-4o': {'input': 5.00, 'output': 15.00, 'context_window': 128, 'training': 'Oct23', 'output': 4, 'desc': 'Text and image input, text output'},\n",
    "    'gpt-4-turbo': {'input': 10.00, 'output': 30.00, 'context_window': 128, 'training': 'Dec23', 'output': 4, 'desc': 'Text and image input, text output'},\n",
    "    'gpt-3.5-turbo': {'input': 0.5, 'output': 1.5, 'context_window': 16, 'training': 'Sep21', 'output': 4, 'desc': 'Text input, text output'},\n",
    "    'text-embedding-3-small': {'input': 0.02, 'output': 0, 'context_window': 16, 'training': '', 'output': 1.5, 'desc': 'convert text into a numerical form'},\n",
    "    'text-embedding-3-large': {'input': 0.13, 'output': 0, 'context_window': 16, 'training': '', 'output': 3, 'desc': 'convert text into a numerical form'},\n",
    "}\n",
    "ModelNames=list(OpenAIModels.keys())\n",
    "DefaultModel = ModelNames[1] # gpt-3.5-turbo\n",
    "DefaultModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 roles:\n",
    "\n",
    "- User - This is meant to mimic the end-user that is interacting with the assistant. This is the role that you will be using most of the time.\n",
    "- System - This role can mimic sort of background nudges and prompts that you might want to inject into the conversation, but that dont need a response. At the moment, system is weighted less than \"user,\" so it still seems more useful to use the user for encouraging specific behaviors in my opinion.\n",
    "- Assistant - This is the agent's response. Often this will be actual responses, but keep in mind... you will be able to inject your own responses here, so you can actually have the agent say whatever you want. This is a bit of a hack, but it's a fun one and can be useful in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the ChatGPT API works is you need to query the model. Since these models often make use of chat history/context, every query needs to, or can, include a full message history context. \n",
    "\n",
    "Keep in mind, however that the maximum context length (XXXX tokens), so you need to stay under that. There are lots of options to work around this, the simplest being truncating earlier messages or to summarize and condense the previous message history. You might use a database or some other storage method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = []\n",
    "\n",
    "def chat(user_input, role=\"user\", model=DefaultModel):\n",
    "    message_history.append({\"role\": role, \"content\": f\"{user_input}\"})\n",
    "    completion = client.chat.completions.create(messages=message_history, model=model)\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})\n",
    "    return reply_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold different scenarios and their estimated output-to-input token ratios\n",
    "token_ratio_estimates = {\n",
    "    'email_response': 1.5,    # Outputs are generally 1.5 times longer than the inputs\n",
    "    'content_summary': 0.8,   # Summaries are usually shorter than the original content\n",
    "    'factual_answer': 0.5,    # Direct answers to factual questions tend to be concise\n",
    "    'creative_story': 3.0,    # Creative stories may be much longer than the initial prompts\n",
    "    'detailed_explanation': 20,  # Detailed explanations or complex answers can be much longer\n",
    "    'limit': 0  # This will be treated specially to limit output tokens\n",
    "}\n",
    "\n",
    "# Maximum tokens for the 'limit' scenario\n",
    "max_output_tokens = 100\n",
    "\n",
    "def calculate_cost(text, model_name = DefaultModel, scenario='limit'):\n",
    "\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    input_tokens = enc.encode(text)   \n",
    "    input_tokens_count = len(input_tokens) \n",
    "\n",
    "    # Determine the output token count based on the scenario\n",
    "    if scenario == 'limit':\n",
    "        output_token_count = max_output_tokens\n",
    "    else:\n",
    "        ratio = token_ratio_estimates.get(scenario, 1)  # Use 1 as a default ratio if the scenario is not found\n",
    "        output_token_count = int(input_tokens_count * ratio)\n",
    "    \n",
    "    # Calculate the total token count (input + estimated output)\n",
    "    total_token_count = input_tokens_count + output_token_count\n",
    "    \n",
    "    # Retrieve cost per million tokens for the model, assuming it can be a dictionary or a single value\n",
    "    cost_per_million = ModelCosts[model_name]\n",
    "    if isinstance(cost_per_million, dict):\n",
    "        # Assuming the model has separate costs for input and output, typically not the case but for example\n",
    "        input_cost_per_million = cost_per_million.get('input', 0)\n",
    "        output_cost_per_million = cost_per_million.get('output', 0)\n",
    "        total_cost = (input_tokens_count / 1_000_000) * input_cost_per_million + (output_token_count / 1_000_000) * output_cost_per_million\n",
    "    else:\n",
    "        total_cost = (total_token_count / 1_000_000) * cost_per_million\n",
    "    \n",
    "    return round(total_cost,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Schema output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9gtWawsMOOE0XTzBUwyGgpGkdpTmH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"world_series_winner\": \"Los Angeles Dodgers\",\\n  \"year\": 2020\\n}', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1720010240, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_abc28019ad', usage=CompletionUsage(completion_tokens=22, prompt_tokens=41, total_tokens=63), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"world_series_winner\": \"Los Angeles Dodgers\",\\n  \"year\": 2020\\n}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"}, # When using JSON mode, always instruct the model to produce JSON via some message in the conversation\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "]\n",
    "response = openai.chat.completions.create(\n",
    "                model=DefaultModel,\n",
    "                messages=messages,\n",
    "                response_format={ \"type\": \"json_object\" }, # When using JSON mode, always instruct the model to produce JSON via some message in the conversation\n",
    "                # seed = 1, # set it to any integer of your choice and use the same value across requests you'd like deterministic outputs for\n",
    "                # temperature=0.5,\n",
    "                # max_tokens=200\n",
    "            )\n",
    "response\n",
    "response.usage.total_tokens\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every response will include a finish_reason. The possible values for finish_reason are:\n",
    "- 'stop': API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n",
    "- 'length': Incomplete model output due to max_tokens parameter or token limit\n",
    "- 'function_call': The model decided to call a function\n",
    "- 'content_filter': Omitted content due to a flag from our content filters\n",
    "- 'null': API response still in progress or incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Date from Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31.12.2012'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n",
    "\n",
    "\n",
    "sys_prompt = '''\n",
    "Please read the filename provided and extract the end date of the quarter in the format DD.MM.YYYY  . The filename will typically indicate the quarter and year, but the name of the file could also have month and a year. Use the following examples for reference:\n",
    "\n",
    "1. 'Fees Q3 2023 Valida corrected.xlsx' should return 30.09.2023\n",
    "2. 'talktalktal 2022Q4_dummytext_not_rrr.xlsx' should return 31.12.2022\n",
    "3. 'Q22023_bla-bla-bla_blaxxxx.xlsx' should return 30.06.2023\n",
    "4. 'dfa fsd sds s September 22' should return 30.09.2022\n",
    "5. 'Somedata_052015_blaxxxx.xlsx' should return 31.05.2015\n",
    "6. 'Data excteract Mar 2021 final' should return 31.03.2021\n",
    "\n",
    "Identify the quarter and year from the filename and return the corresponding end date of that quarter.\n",
    "'''\n",
    "\n",
    "name_of_file = 'sdfsfdsfd2012Q4_dummytext_not_rrr.xlsx'\n",
    "#name_of_file = 'F 2014 Q1 Valida corrected.xlsx'\n",
    "#name_of_file = 'File corrected Mar 2022.xlsx'\n",
    "\n",
    "messages = [\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": name_of_file}\n",
    "            ]\n",
    "# os.environ['HTTPS_PROXY'] = ''\n",
    "response = openai.chat.completions.create(\n",
    "                model='gpt-4',\n",
    "                messages=messages,\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            )\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AOAI_ENDPOINT\"),\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    api_key=os.getenv(\"AOAI_KEY\")\n",
    ")\n",
    "GPT_MODEL = \"gpt-35-turbo\"\n",
    "\n",
    "# Sample input from Chapter 1:\n",
    "#Hello. My name is Amit Bahree. Iâ€™m calling from Acme Insurance, Bellevue, WA. My colleague mentioned that you are interested in learning about our comprehensive benefits policy. Could you give me a call back at (555) 111-2222 when you get a chance so we can go over the benefits? I can be reached Monday to Friday during normal business hours of PST. If you want you can also try and reach me on emails at aweomseinsrance@acme.com. Thanks, Amit.\n",
    "\n",
    "conversation=[{\"role\": \"system\", \"content\": \"You are an AI assistant that extracts entities from text as JSON. \\nHere is an example of your output format:\\n{  \\n   \\\"the_name\\\": \\\"\\\",\\n   \\\"the_company\\\": \\\"\\\",\\n   \\\"a_phone_number\\\": \\\"\\\"\\n}\"}]\n",
    "print(\"Please enter what you want to talk about:\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = GPT_MODEL,\n",
    "        messages = conversation)\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\nAI:\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm # for progress bars\n",
    "import tiktoken as tk\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_BOOK_KEY\"))\n",
    "\n",
    "# function that splits the text into chunks based on sentences\n",
    "def split_sentences(text):\n",
    "    sentences = re.split('[.!?]', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence]\n",
    "    return sentences\n",
    "\n",
    "# count tokens\n",
    "def count_tokens(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    # Get the encoding\n",
    "    encoding = tk.get_encoding(encoding_name)\n",
    "    \n",
    "    # Encode the string\n",
    "    encoded_string = encoding.encode(string)\n",
    "\n",
    "    # Count the number of tokens\n",
    "    num_tokens = len(encoded_string)\n",
    "    return num_tokens\n",
    "\n",
    "# OpenAI embeddings example from Chapter 2\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    text = \"This is the first sentence. This is the second sentence. Guess what? This is the fourth sentence.\"\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    # Initialize an empty 2D array\n",
    "    sentence_embeddings = []\n",
    "    total_token_count = 0\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        # Count the number of tokens in the sentence\n",
    "        total_token_count += count_tokens(sentence, \"cl100k_base\")\n",
    "        \n",
    "        # Append the sentence and its embedding to the 2D array\n",
    "        embedding = get_embedding(sentence)\n",
    "        sentence_embeddings.append([sentence, embedding])\n",
    "\n",
    "    # Now, sentence_embeddings is a 2D array where each element is a list of the form [sentence, embedding]\n",
    "    print(\"Number of sentence embeddings:\", len(sentence_embeddings))\n",
    "    print(\"Total number of tokens:\", total_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to tokenize the text into sentences.\n",
    "# Use tiktoken to count the tokens accurately.\n",
    "# Slide through the sentences using a window (defined by the token limit), optionally allowing overlaps.\n",
    "\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import tiktoken as tk\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AOAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AOAI_ENDPOINT\"),\n",
    "    api_version=\"2022-12-01\")\n",
    "\n",
    "# count tokens\n",
    "def count_tokens(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    # Get the encoding\n",
    "    encoding = tk.get_encoding(encoding_name)\n",
    "    \n",
    "    # Encode the string\n",
    "    encoded_string = encoding.encode(string)\n",
    "\n",
    "    # Count the number of tokens\n",
    "    num_tokens = len(encoded_string)\n",
    "    return num_tokens\n",
    "\n",
    "# OpenAI embeddings example from Chapter 2\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"ada-embedding\",\n",
    "        input=text)\n",
    "    \n",
    "    return response.data[0].embedding\n",
    "\n",
    "# function that splits the text into chunks based on sentences\n",
    "def chunking_with_spacy(text, max_tokens, \n",
    "                        overlap=0, \n",
    "                        model=\"en_core_web_sm\"):\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(model)\n",
    "    \n",
    "    # Tokenize the text into sentences using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    # Tokenize sentences into tokens and accumulate tokens\n",
    "    tokens_lengths = [count_tokens(sent) for sent in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(sentences):\n",
    "        current_chunk = []\n",
    "        current_token_count = 0\n",
    "        for idx in range(start_idx, len(sentences)):\n",
    "            if current_token_count + tokens_lengths[idx] > max_tokens:\n",
    "                break\n",
    "            current_chunk.append(sentences[idx])\n",
    "            current_token_count += tokens_lengths[idx]\n",
    "        \n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        # Sliding window adjustment\n",
    "        if overlap >= len(current_chunk):\n",
    "            start_idx += 1\n",
    "        else:\n",
    "            start_idx += len(current_chunk) - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example\n",
    "    text = (\"This is a demonstration of text chunking with spaCy and tiktoken. \"\n",
    "        \"Using both allows for precise token counting and effective chunking. \"\n",
    "        \"Overlap and sliding window strategies are useful for various applications. \"\n",
    "        \"Choose your strategy based on your requirements.\")\n",
    "\n",
    "    max_tokens = 25\n",
    "    overlap_sentences = 2\n",
    "    chunks = chunking_with_spacy(text, max_tokens, overlap=overlap_sentences)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i + 1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "import time\n",
    "import textwrap\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm # for progress bars\n",
    "import tiktoken as tk\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AOAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AOAI_ENDPOINT\"),\n",
    "    api_version=\"2024-05-01-preview\")\n",
    "\n",
    "GPT_MODEL = \"gpt-35-turbo\"\n",
    "\n",
    "# function that splits the text into chunks based on sentences\n",
    "def split_sentences(text):\n",
    "    sentences = re.split('[.!?]', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence]\n",
    "    return sentences\n",
    "\n",
    "# function that splits the text into chunks based on sentences\n",
    "def split_sentences_by_textwrap(text):\n",
    "    # set the maximum chunk size to 2048 characters\n",
    "    max_chunk_size = 2048\n",
    "    # use the wrap function to split the text into chunks\n",
    "    chunks = textwrap.wrap(\n",
    "        text, \n",
    "        width=max_chunk_size,\n",
    "        break_long_words=False, \n",
    "        break_on_hyphens=False)\n",
    "    # return the list of chunks\n",
    "    return chunks\n",
    "\n",
    "def split_sentences_by_nltk(text):\n",
    "  chunks = []\n",
    "\n",
    "  for sentence in nltk.sent_tokenize(text):\n",
    "    #num_tokens_in_sentence = len(nltk.word_tokenize(sentence))\n",
    "    #print(sentence)\n",
    "    chunks.append(sentence)\n",
    "    \n",
    "  return chunks\n",
    "\n",
    "def split_sentences_by_spacy(text, max_tokens, \n",
    "                        overlap=0,\n",
    "                        model=\"en_core_web_sm\"):\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(model)\n",
    "    \n",
    "    # Tokenize the text into sentences using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    # Tokenize sentences into tokens and accumulate tokens\n",
    "    tokens_lengths = [count_tokens(sent) for sent in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(sentences):\n",
    "        current_chunk = []\n",
    "        current_token_count = 0\n",
    "        for idx in range(start_idx, len(sentences)):\n",
    "            if current_token_count + tokens_lengths[idx] > max_tokens:\n",
    "                break\n",
    "            current_chunk.append(sentences[idx])\n",
    "            current_token_count += tokens_lengths[idx]\n",
    "        \n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        # Sliding window adjustment\n",
    "        if overlap >= len(current_chunk):\n",
    "            start_idx += 1\n",
    "        else:\n",
    "            start_idx += len(current_chunk) - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# count tokens\n",
    "def count_tokens(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    # Get the encoding\n",
    "    encoding = tk.get_encoding(encoding_name)\n",
    "    \n",
    "    # Encode the string\n",
    "    encoded_string = encoding.encode(string)\n",
    "\n",
    "    # Count the number of tokens\n",
    "    num_tokens = len(encoded_string)\n",
    "    return num_tokens\n",
    "\n",
    "# OpenAI embeddings example from Chapter 2\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(model=\"ada-embedding\",\n",
    "    input=text)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def generate_summaries(chunks):\n",
    "    # create an empty list to store the summaries\n",
    "    summaries = []\n",
    "    \n",
    "    # loop through each chunk\n",
    "    for chunk in tqdm(chunks):\n",
    "        # create a prompt that instructs the model to summarize the chunk\n",
    "        prompt = f\"Summarize the following text in one sentence:\\n{chunk}\\nSummary:\"\n",
    "        \n",
    "        # use the OpenAI.Completion class to generate a summary for the chunk\n",
    "        response = client.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            prompt=prompt,\n",
    "            max_tokens=1500,\n",
    "            temperature=0.7)\n",
    "        \n",
    "        # get the summary from the response\n",
    "        #summary = response[\"choices\"][0][\"text\"]\n",
    "        summary = response.choices[0].text\n",
    "        # append the summary to the list of summaries\n",
    "        summaries.append(summary)\n",
    "        sleep(1) # sleep for 1 second(s) for rate limiting\n",
    "\n",
    "    # return the list of summaries\n",
    "    return summaries\n",
    "\n",
    "def process_chunks(sentences):\n",
    "    sentence_embeddings = []\n",
    "    total_token_count = 0\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        # Count the number of tokens in the sentence\n",
    "        total_token_count += count_tokens(sentence, \"cl100k_base\")\n",
    "        \n",
    "        # Append the sentence and its embedding to the 2D array\n",
    "        embedding = get_embedding(sentence)\n",
    "        sentence_embeddings.append([sentence, embedding])\n",
    "\n",
    "    #print(\"Simple Sentence Chunking:\")\n",
    "    print(\"\\tNumber of sentence embeddings:\", len(sentence_embeddings))\n",
    "    print(\"\\tTotal number of tokens:\", total_token_count)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "def main():\n",
    "    # load a text file that you want to chunk\n",
    "    TEXT_FILE = \"./data/women_fifa_worldcup_2023.txt\"\n",
    "    \n",
    "    print(\"Reading the file ...\")\n",
    "    \n",
    "    # read the text from the file\n",
    "    with open(TEXT_FILE, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(\"1. Simple sentence chunking ...\")\n",
    "    start_time = time.time()\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    #print(\"Number of sentences:\", len(sentences))\n",
    "    process_chunks(sentences)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    # ===================================\n",
    "\n",
    "    #Reset variables\n",
    "    summaries = []\n",
    "    sentences = []\n",
    "    sentence_embeddings = []\n",
    "    total_token_count = 0\n",
    "    chunks = []\n",
    "\n",
    "    print(\"2. Sentence chunking using textwrap ...\")\n",
    "    start_time = time.time()\n",
    "    # split the text into chunks by sentences\n",
    "    chunks = split_sentences_by_textwrap(text)\n",
    "    #print(f\"Number of chunks: {len(chunks)}\")\n",
    "    process_chunks(chunks)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    # ===================================\n",
    "\n",
    "    #Reset variables\n",
    "    summaries = []\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "\n",
    "    print(\"3. Sentence chunking using NLTK ...\")\n",
    "    # split the text into chunks by sentences\n",
    "    chunks = split_sentences_by_nltk(text)\n",
    "    #print(f\"Number of chunks: {len(chunks)}\")\n",
    "    start_time = time.time()\n",
    "    process_chunks(chunks)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    # ===================================\n",
    "\n",
    "    #Reset variables\n",
    "    summaries = []\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "\n",
    "    print(\"4. Sentence chunking using spaCy ...\")\n",
    "    # split the text into chunks by sentences\n",
    "    start_time = time.time()\n",
    "    chunks = split_sentences_by_spacy(text, max_tokens=2000, overlap=0)\n",
    "    #print(f\"Number of chunks: {len(chunks)}\")\n",
    "    process_chunks(chunks)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    # ===================================\n",
    "    # generate summaries for each chunk using OpenAI API\n",
    "    summaries = generate_summaries(chunks)\n",
    "    print(\"Summaries generated by OpenAI API:\")\n",
    "    # print the summaries\n",
    "    print(summaries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
