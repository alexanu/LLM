{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['OPENAI_API_KEY'])\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the ChatGPT API works is you need to query the model. Since these models often make use of chat history/context, every query needs to, or can, include a full message history context. \n",
    "\n",
    "Keep in mind, however that the maximum context length is 4096 tokens, so you need to stay under that. There are lots of options to work around this, the simplest being truncating earlier messages or to summarize and condense the previous message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCosts = {\n",
    "    'gpt-4-128k': {'input': 10.00, 'output': 30.00},\n",
    "    'gpt-4-8k': {'input': 30.00, 'output': 60.00},\n",
    "    'gpt-4-32k': {'input': 60.00, 'output': 120.00},\n",
    "    'gpt-3.5-turbo-1106': {'input': 1.00, 'output': 2.00},\n",
    "    'gpt-3.5-turbo-0613': {'input': 1.50, 'output': 2.00},\n",
    "    'gpt-3.5-turbo-16k-0613': {'input': 3.00, 'output': 4.00},\n",
    "    'gpt-3.5-turbo-0301': {'input': 1.50, 'output': 2.00},\n",
    "    'davinci-002': 2.00,\n",
    "    'babbage-002': 0.40,\n",
    "    'text-embedding-3-small': 0.02,\n",
    "    'text-embedding-3-large': 0.13,\n",
    "    'ada v2': 0.10,\n",
    "}\n",
    "ModelNames=list(ModelCosts.keys())\n",
    "DefaultModel = ModelNames[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold different scenarios and their estimated output-to-input token ratios\n",
    "token_ratio_estimates = {\n",
    "    'email_response': 1.5,    # Outputs are generally 1.5 times longer than the inputs\n",
    "    'content_summary': 0.8,   # Summaries are usually shorter than the original content\n",
    "    'factual_answer': 0.5,    # Direct answers to factual questions tend to be concise\n",
    "    'creative_story': 3.0,    # Creative stories may be much longer than the initial prompts\n",
    "    'detailed_explanation': 20,  # Detailed explanations or complex answers can be much longer\n",
    "    'limit': 0  # This will be treated specially to limit output tokens\n",
    "}\n",
    "\n",
    "# Maximum tokens for the 'limit' scenario\n",
    "max_output_tokens = 100\n",
    "\n",
    "def calculate_cost(text, model_name = DefaultModel, scenario='limit'):\n",
    "\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    input_tokens = enc.encode(text)   \n",
    "    input_tokens_count = len(input_tokens) \n",
    "\n",
    "    # Determine the output token count based on the scenario\n",
    "    if scenario == 'limit':\n",
    "        output_token_count = max_output_tokens\n",
    "    else:\n",
    "        ratio = token_ratio_estimates.get(scenario, 1)  # Use 1 as a default ratio if the scenario is not found\n",
    "        output_token_count = int(input_tokens_count * ratio)\n",
    "    \n",
    "    # Calculate the total token count (input + estimated output)\n",
    "    total_token_count = input_tokens_count + output_token_count\n",
    "    \n",
    "    # Retrieve cost per million tokens for the model, assuming it can be a dictionary or a single value\n",
    "    cost_per_million = ModelCosts[model_name]\n",
    "    if isinstance(cost_per_million, dict):\n",
    "        # Assuming the model has separate costs for input and output, typically not the case but for example\n",
    "        input_cost_per_million = cost_per_million.get('input', 0)\n",
    "        output_cost_per_million = cost_per_million.get('output', 0)\n",
    "        total_cost = (input_tokens_count / 1_000_000) * input_cost_per_million + (output_token_count / 1_000_000) * output_cost_per_million\n",
    "    else:\n",
    "        total_cost = (total_token_count / 1_000_000) * cost_per_million\n",
    "    \n",
    "    return round(total_cost,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Say this is a test\"\n",
    "chat_completion = client.chat.completions.create(messages=[{\"role\": \"user\",\"content\": message,}],model=DefaultModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.usage.total_tokens\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles\n",
    "There are 3 roles:\n",
    "\n",
    "- User - This is meant to mimic the end-user that is interacting with the assistant. This is the role that you will be using most of the time.\n",
    "- System - This role can mimic sort of background nudges and prompts that you might want to inject into the conversation, but that dont need a response. At the moment, system is weighted less than \"user,\" so it still seems more useful to use the user for encouraging specific behaviors in my opinion.\n",
    "- Assistant - This is the agent's response. Often this will be actual responses, but keep in mind... you will be able to inject your own responses here, so you can actually have the agent say whatever you want. This is a bit of a hack, but it's a fun one and can be useful in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "API itself doesn't manage your history. For now - essage history variable, but you might use a database or some other storage method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = []\n",
    "\n",
    "def chat(user_input, role=\"user\", model=DefaultModel):\n",
    "    message_history.append({\"role\": role, \"content\": f\"{user_input}\"})\n",
    "    completion = client.chat.completions.create(messages=message_history, model=model)\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})\n",
    "    return reply_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
